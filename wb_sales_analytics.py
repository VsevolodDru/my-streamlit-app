import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import json
from datetime import datetime, timedelta
import pytz
import numpy as np
import io
from openpyxl import Workbook
import requests
import gc
import logging
from typing import Optional, Tuple
import tempfile

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    layout="wide",
    page_title="WB Analytics Pro",
    page_icon="üìà",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://example.com',
        'Report a bug': "https://example.com",
        'About': "# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ Wildberries"
    }
)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
MAX_JSON_SIZE_MB = 500
JSON_LOAD_TIMEOUT = 600
CHUNK_SIZE = 1024 * 1024

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
global_df = None
global_excel_df = None

@st.cache_data(ttl=3600, max_entries=3, show_spinner="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
def load_large_json(url: str) -> pd.DataFrame:
    try:
        logger.info(f"–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ JSON —Ñ–∞–π–ª–∞ –∏–∑ {url}")
        
        with requests.head(url, timeout=10) as r:
            size_mb = int(r.headers.get('content-length', 0)) / (1024 * 1024)
            if size_mb > MAX_JSON_SIZE_MB:
                st.warning(f"‚ö†Ô∏è –§–∞–π–ª –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π ({size_mb:.1f} –ú–ë). –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç...")

        progress_bar = st.progress(0)
        status_text = st.empty()
        
        response = requests.get(url, stream=True, timeout=(30, JSON_LOAD_TIMEOUT))
        response.raise_for_status()
        
        chunks = []
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        for chunk in response.iter_content(chunk_size=CHUNK_SIZE):
            chunks.append(chunk)
            downloaded += len(chunk)
            progress = min(downloaded / total_size, 1.0) if total_size > 0 else 0
            progress_bar.progress(progress)
            status_text.text(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {downloaded/(1024*1024):.1f} –ú–ë / {total_size/(1024*1024):.1f} –ú–ë")
        
        status_text.text("–û–±—Ä–∞–±–æ—Ç–∫–∞ JSON...")
        data = json.loads(b''.join(chunks).decode('utf-8'))
        df = pd.DataFrame(data)
        
        datetime_cols = ['date', 'lastChangeDate']
        for col in datetime_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col].dt.tz_localize('Europe/Moscow'))
        
        df['is_return'] = df.get('srid', '').str.startswith('R')
        df['revenue'] = df['totalPrice']
        df['week'] = df['date'].dt.isocalendar().week
        df['month'] = df['date'].dt.month
        df['isCancel'] = df.get('isCancel', False)

        column_mapping = {
            'date': '–î–∞—Ç–∞',
            'warehouseType': '–°–∫–ª–∞–¥',
            'regionName': '–†–µ–≥–∏–æ–Ω',
            'category': '–ö–∞—Ç–µ–≥–æ—Ä–∏—è',
            'brand': '–ë—Ä–µ–Ω–¥',
            'subject': '–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è',
            'totalPrice': '–¶–µ–Ω–∞',
            'revenue': '–í—ã—Ä—É—á–∫–∞',
            'spp': '–°–ü–ü',
            'supplierArticle': '–ê—Ä—Ç–∏–∫—É–ª'
        }
        df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})

        str_cols = ['–ë—Ä–µ–Ω–¥', '–ê—Ä—Ç–∏–∫—É–ª', '–ö–∞—Ç–µ–≥–æ—Ä–∏—è', '–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è']
        for col in str_cols:
            if col in df.columns:
                df[col] = df[col].astype('string')
        
        df['–ë—Ä–µ–Ω–¥'] = df['–ë—Ä–µ–Ω–¥'].str.lower()
        
        if '–ê—Ä—Ç–∏–∫—É–ª' in df.columns:
            df['–ê—Ä—Ç–∏–∫—É–ª'] = df['–ê—Ä—Ç–∏–∫—É–ª'].apply(
                lambda x: x[:len(x)//2] if len(x) == 20 and x[:10] == x[10:] else x
            )

        logger.info(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
        return df
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}", exc_info=True)
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        return pd.DataFrame()
    finally:
        if 'progress_bar' in locals(): progress_bar.empty()
        if 'status_text' in locals(): status_text.empty()

@st.cache_data(ttl=3600, max_entries=2)
def load_excel_data(url: str) -> pd.DataFrame:
    try:
        logger.info("–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ Excel –¥–∞–Ω–Ω—ã—Ö")
        response = requests.get(url, timeout=(30, 300))
        response.raise_for_status()
        
        with io.BytesIO(response.content) as excel_file:
            df = pd.read_excel(
                excel_file,
                usecols=['–ê—Ä—Ç–∏–∫—É–ª –ø—Ä–æ–¥–∞–≤—Ü–∞', '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ'],
                dtype={'–ê—Ä—Ç–∏–∫—É–ª –ø—Ä–æ–¥–∞–≤—Ü–∞': 'string', '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ': 'string'}
            )
        
        if '–ê—Ä—Ç–∏–∫—É–ª –ø—Ä–æ–¥–∞–≤—Ü–∞' not in df.columns or '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ' not in df.columns:
            raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ Excel —Ñ–∞–π–ª–µ")
        
        df = df.rename(columns={
            '–ê—Ä—Ç–∏–∫—É–ª –ø—Ä–æ–¥–∞–≤—Ü–∞': '–ê—Ä—Ç–∏–∫—É–ª',
            '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ': '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞'
        })
        
        logger.info(f"Excel –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –†–∞–∑–º–µ—Ä: {len(df)} —Å—Ç—Ä–æ–∫")
        return df[['–ê—Ä—Ç–∏–∫—É–ª', '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞']]
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Excel: {str(e)}", exc_info=True)
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ Excel —Ñ–∞–π–ª–∞: {str(e)}")
        return pd.DataFrame()

def to_excel(df: pd.DataFrame) -> bytes:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç DataFrame –≤ Excel —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏."""
    try:
        df_copy = df.copy()
        
        # –û—á–∏—Å—Ç–∫–∞ datetime –æ–±—ä–µ–∫—Ç–æ–≤
        datetime_cols = ['–î–∞—Ç–∞', 'lastChangeDate']
        for col in datetime_cols:
            if col in df_copy.columns:
                df_copy[col] = df_copy[col].dt.tz_localize(None)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±—É—Ñ–µ—Ä
        output = io.BytesIO()
        
        with pd.ExcelWriter(
            output,
            engine='openpyxl'
        ) as writer:
            df_copy.to_excel(
                writer,
                index=False,
                sheet_name='SalesData'
            )
        
        return output.getvalue()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Excel: {str(e)}", exc_info=True)
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ Excel —Ñ–∞–π–ª–∞: {str(e)}")
        raise

def apply_filters(df: pd.DataFrame, date_range: Tuple[datetime.date, datetime.date], 
                 include_cancelled: bool, warehouse_type: list) -> pd.DataFrame:
    try:
        filtered = df[
            (df['–î–∞—Ç–∞'].dt.date >= date_range[0]) &
            (df['–î–∞—Ç–∞'].dt.date <= date_range[1]) &
            (~df['is_return'])
        ].copy()
        
        if not include_cancelled:
            filtered = filtered[~filtered['isCancel']]
            
        if warehouse_type:
            filtered = filtered[filtered['–°–∫–ª–∞–¥'].isin(warehouse_type)]
        
        for col in filtered.select_dtypes(include=['object']):
            filtered[col] = filtered[col].astype('string')
            
        return filtered
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {str(e)}", exc_info=True)
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        raise

def main():
    global global_df, global_excel_df
    
    st.title("üîç Wildberries Analytics Pro (Large Files Support)")
    
    json_url = "https://storage.yandexcloud.net/my-json-bucket-chat-wb/wb_dashboard/all_sales_data.json"
    excel_url = "https://storage.yandexcloud.net/my-json-bucket-chat-wb/14_04_2025_07_26_%D0%9E%D0%B1%D1%89%D0%B8%D0%B5_%D1%85%D0%B0%D1%80%D0%B0%D0%BA%D1%82%D0%B5%D1%80%D0%B8%D1%81%D1%82%D0%B8%D0%BA%D0%B8_%D0%BE%D0%B4%D0%BD%D0%B8%D0%BC_%D1%84%D0%B0%D0%B9%D0%BB%D0%BE%D0%BC.xlsx"
    
    if 'data_loaded' not in st.session_state:
        with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤)..."):
            try:
                global_df = load_large_json(json_url)
                
                if global_df is not None and not global_df.empty:
                    global_excel_df = load_excel_data(excel_url)
                    
                    if global_excel_df is not None and not global_excel_df.empty:
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                        duplicates = global_excel_df.duplicated(subset=['–ê—Ä—Ç–∏–∫—É–ª']).sum()
                        if duplicates > 0:
                            st.warning(f"–ù–∞–π–¥–µ–Ω–æ {duplicates} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∞—Ä—Ç–∏–∫—É–ª–æ–≤ –≤ Excel —Ñ–∞–π–ª–µ. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–µ—Ä–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.")
                            global_excel_df = global_excel_df.drop_duplicates(subset=['–ê—Ä—Ç–∏–∫—É–ª'], keep='first')
                        
                        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
                        global_df = pd.merge(
                            global_df,
                            global_excel_df,
                            on='–ê—Ä—Ç–∏–∫—É–ª',
                            how='left'
                        )
                        st.session_state.data_loaded = True
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
                return
    
    if global_df is None or global_df.empty:
        st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
        return
    
    if st.button("üîÑ –°–±—Ä–æ—Å–∏—Ç—å –∫—ç—à –∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ"):
        st.cache_data.clear()
        st.session_state.clear()
        global_df = None
        global_excel_df = None
        st.experimental_rerun()
    
    min_date = global_df['–î–∞—Ç–∞'].min().date()
    max_date = global_df['–î–∞—Ç–∞'].max().date()
    
    with st.sidebar:
        st.header("‚è± –ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞")
        try:
            date_range = st.date_input(
                "–í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç—ã",
                [min_date, max_date],
                min_value=min_date,
                max_value=max_date,
                format="DD.MM.YYYY"
            )
            if len(date_range) != 2:
                st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç")
                st.stop()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–±–æ—Ä–∞ –¥–∞—Ç—ã: {str(e)}", exc_info=True)
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –¥–∞—Ç—ã: {str(e)}")
            st.stop()
        
        include_cancelled = st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å –æ—Ç–º–µ–Ω—ã", value=False)
        st.header("üóÇ –§–∏–ª—å—Ç—Ä—ã")
        warehouse_type = st.multiselect(
            "–¢–∏–ø —Å–∫–ª–∞–¥–∞",
            options=global_df['–°–∫–ª–∞–¥'].unique(),
            default=global_df['–°–∫–ª–∞–¥'].unique()[0] if len(global_df['–°–∫–ª–∞–¥'].unique()) > 0 else []
        )
    
    if 'filtered_df' not in st.session_state or st.button("–ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã"):
        with st.spinner("–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤..."):
            try:
                st.session_state.filtered_df = apply_filters(
                    global_df,
                    date_range,
                    include_cancelled,
                    warehouse_type
                )
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {str(e)}")
                st.stop()
    
    filtered_df = st.session_state.get('filtered_df', pd.DataFrame())
    
    if filtered_df.empty:
        st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ñ–∏–ª—å—Ç—Ä–∞–º")
        st.stop()
    
    st.subheader("üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    cols = st.columns(3)
    cols[0].metric("–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π", len(filtered_df))
    cols[1].metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤", filtered_df['srid'].nunique())
    cols[2].metric("–î—É–±–ª–∏–∫–∞—Ç–æ–≤ srid", filtered_df.duplicated(subset=['srid']).sum())
    
    st.header("üìä –ö–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏")
    
    with st.spinner("–†–∞—Å—á–µ—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π..."):
        try:
            revenue = filtered_df['–í—ã—Ä—É—á–∫–∞'].sum()
            order_count = filtered_df['srid'].nunique()
            avg_check = revenue / order_count if order_count > 0 else 0
            avg_spp = filtered_df['–°–ü–ü'].mean()
            
            cols = st.columns(4)
            cols[0].metric("–í—ã—Ä—É—á–∫–∞", f"{revenue:,.0f} ‚ÇΩ")
            cols[1].metric("–°—Ä–µ–¥–Ω–∏–π —á–µ–∫", f"{avg_check:,.0f} ‚ÇΩ")
            cols[2].metric("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–∫–∞–∑–æ–≤", order_count)
            cols[3].metric("–°—Ä–µ–¥–Ω–∏–π –°–ü–ü", 
                          f"{avg_spp:.2f}%" if not pd.isna(avg_spp) else "N/A",
                          help="–°—Ä–µ–¥–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —Å–∫–∏–¥–∫–∏ –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫: {str(e)}", exc_info=True)
            st.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π")
    
    tab1, tab2 = st.tabs(["üìà –î–∏–Ω–∞–º–∏–∫–∞ –ø—Ä–æ–¥–∞–∂", "üí∞ –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã—Ä—É—á–∫–∏"])
    
    with tab1:
        st.subheader("–î–∏–Ω–∞–º–∏–∫–∞ –ø—Ä–æ–¥–∞–∂")
        try:
            freq = st.radio("–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞", ["–î–µ–Ω—å", "–ù–µ–¥–µ–ª—è", "–ú–µ—Å—è—Ü"], 
                          horizontal=True, key="freq_selector")
            freq_map = {"–î–µ–Ω—å": "D", "–ù–µ–¥–µ–ª—è": "W", "–ú–µ—Å—è—Ü": "ME"}
            
            with st.spinner("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞..."):
                dynamic_df = filtered_df.groupby(
                    pd.Grouper(key='–î–∞—Ç–∞', freq=freq_map[freq])
                ).agg({
                    '–í—ã—Ä—É—á–∫–∞': 'sum',
                    'srid': 'nunique'
                }).reset_index()
                
                fig = go.Figure()
                fig.add_trace(go.Scatter(
                    x=dynamic_df['–î–∞—Ç–∞'],
                    y=dynamic_df['–í—ã—Ä—É—á–∫–∞'],
                    name="–í—ã—Ä—É—á–∫–∞",
                    line=dict(color='#1f77b4', width=2)
                ))
                fig.add_trace(go.Scatter(
                    x=dynamic_df['–î–∞—Ç–∞'],
                    y=dynamic_df['srid'],
                    name="–ó–∞–∫–∞–∑—ã",
                    line=dict(color='#ff7f0e', width=2),
                    yaxis="y2"
                ))
                
                fig.update_layout(
                    title=f"–î–∏–Ω–∞–º–∏–∫–∞ –ø–æ {freq.lower()}–º",
                    yaxis=dict(title="–í—ã—Ä—É—á–∫–∞ (‚ÇΩ)"),
                    yaxis2=dict(title="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–∫–∞–∑–æ–≤", overlaying="y", side="right"),
                    hovermode="x unified",
                    legend=dict(orientation="h", y=1.1)
                )
                st.plotly_chart(fig, use_container_width=True)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞: {str(e)}", exc_info=True)
            st.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –¥–∏–Ω–∞–º–∏–∫–∏ –ø—Ä–æ–¥–∞–∂")
    
    with tab2:
        st.subheader("–î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã—Ä—É—á–∫–∏")
        
        if st.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é", True, key="show_details"):
            with st.spinner("–ê–Ω–∞–ª–∏–∑ –≤—ã—Ä—É—á–∫–∏..."):
                try:
                    total_revenue = filtered_df['–í—ã—Ä—É—á–∫–∞'].sum()
                    
                    def display_revenue_analysis(df, group_col, title):
                        analysis_df = df.groupby(group_col).agg({
                            '–í—ã—Ä—É—á–∫–∞': ['sum', 'count'],
                            '–°–ü–ü': 'mean'
                        }).reset_index()
                        
                        analysis_df.columns = [group_col, '–í—ã—Ä—É—á–∫–∞', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–°—Ä–µ–¥–Ω–∏–π –°–ü–ü']
                        analysis_df['–î–æ–ª—è'] = (analysis_df['–í—ã—Ä—É—á–∫–∞'] / total_revenue) * 100
                        analysis_df['–°—Ä–µ–¥–Ω–∏–π –°–ü–ü'] = analysis_df['–°—Ä–µ–¥–Ω–∏–π –°–ü–ü'].round(2)
                        
                        st.subheader(title)
                        fig = px.bar(
                            analysis_df,
                            x=group_col,
                            y='–í—ã—Ä—É—á–∫–∞',
                            hover_data=['–î–æ–ª—è', '–°—Ä–µ–¥–Ω–∏–π –°–ü–ü'],
                            labels={'–í—ã—Ä—É—á–∫–∞': '–í—ã—Ä—É—á–∫–∞, ‚ÇΩ'},
                            title=title
                        )
                        st.plotly_chart(fig, use_container_width=True)
                        
                        analysis_df = analysis_df.sort_values('–í—ã—Ä—É—á–∫–∞', ascending=False)
                        st.dataframe(analysis_df)
                        
                        st.download_button(
                            label=f"–°–∫–∞—á–∞—Ç—å {title.lower()}",
                            data=to_excel(analysis_df),
                            file_name=f"{title.replace(' ', '_')}.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )
                        return analysis_df
                    
                    cat_df = display_revenue_analysis(filtered_df, '–ö–∞—Ç–µ–≥–æ—Ä–∏—è', "–í—ã—Ä—É—á–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
                    selected_cat = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—é", cat_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'].unique())
                    
                    cat_details = filtered_df[filtered_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'] == selected_cat]
                    subcat_df = display_revenue_analysis(cat_details, '–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è', 
                                                       f"–í—ã—Ä—É—á–∫–∞ –ø–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º ({selected_cat})")
                    
                    brand_df = display_revenue_analysis(filtered_df, '–ë—Ä–µ–Ω–¥', "–í—ã—Ä—É—á–∫–∞ –ø–æ –±—Ä–µ–Ω–¥–∞–º")
                    
                    if date_range[0] == date_range[1]:
                        st.subheader("–ü–æ—á–∞—Å–æ–≤–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞")
                        hourly_df = filtered_df.groupby(filtered_df['–î–∞—Ç–∞'].dt.hour).agg({
                            '–í—ã—Ä—É—á–∫–∞': 'sum',
                            'srid': 'nunique'
                        }).reset_index().rename(columns={'–î–∞—Ç–∞': '–ß–∞—Å'})
                        
                        fig = px.bar(
                            hourly_df,
                            x='–ß–∞—Å',
                            y='–í—ã—Ä—É—á–∫–∞',
                            hover_data=['srid'],
                            labels={'srid': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–∫–∞–∑–æ–≤'},
                            title='–í—ã—Ä—É—á–∫–∞ –ø–æ —á–∞—Å–∞–º'
                        )
                        st.plotly_chart(fig, use_container_width=True)
                        
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏: {str(e)}", exc_info=True)
                    st.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –≤—ã—Ä—É—á–∫–∏")
    
    with st.expander("üìÅ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö", expanded=False):
        st.subheader("–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        st.dataframe(
            filtered_df.head(1000),
            height=400,
            use_container_width=True
        )
        
        cols = st.columns(2)
        cols[0].download_button(
            label="üì• Excel (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)",
            data=to_excel(filtered_df),
            file_name="wb_analytics.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        cols[1].download_button(
            label="üì• CSV (—Å–∂–∞—Ç—ã–π)",
            data=filtered_df.to_csv(index=False, encoding='utf-8').encode('utf-8'),
            file_name="wb
